# 제품 요구사항 정의서 (PRD)

## 프로젝트명: Zero-Watchdog (KEDA Internal Traffic Sensor)

| 항목 | 내용 |
| :--- | :--- |
| **작성일** | 2025년 12월 9일 |
| **목표** | KEDA Zero Scaling($0 \to 1$)을 위한 초경량 내부 트래픽 감지 eBPF 에이전트 개발 |
| **대상 플랫폼** | Kubernetes Linux Nodes (x86\_64/arm64) |
| **핵심 기술** | eBPF (Kprobe), Go (User Space), Prometheus Metrics |

-----

## 1\. 개요 및 배경 (Overview)

### 1.1 배경

KEDA와 VictoriaMetrics Single(Edge Mode)을 사용하는 환경에서, 파드(Pod)가 0개(Scaled to Zero)일 때 발생하는 **내부 서비스 간(Service-to-Service) 트래픽**은 목적지가 없어 커널 레벨에서 즉시 폐기된다. 이로 인해 Node Exporter나 Kube-state-metrics로는 트래픽 발생 사실을 감지할 수 없어 오토스케일링이 트리거되지 않는 문제가 있다.

### 1.2 목표

  * 기존의 무거운 Observability 도구(Istio, Cilium, Beyla) 없이 **L4(TCP) 연결 시도**만을 감지하는 단일 목적의 에이전트를 개발한다.
  * 바이너리 크기 20MB 이하, 메모리 사용량 50MB 이하의 **초경량 구조**를 지향한다.
  * 감지된 트래픽을 KEDA가 인식할 수 있는 **Prometheus 메트릭**으로 변환하여 노출한다.

-----

## 2\. 시스템 아키텍처 (Architecture)

시스템은 커널 영역(Kernel Space)과 사용자 영역(User Space)으로 나뉘며, 고속 데이터 전송을 위해 **eBPF RingBuffer**를 사용한다.

### 2.1 구성 요소

1.  **Kernel Probe (C언어):** `tcp_v4_connect` 시스템 콜을 후킹하여 모든 TCP 연결 시도의 \*\*목적지 IP(Destination IP)\*\*를 캡처한다.
2.  **RingBuffer:** 커널에서 캡처한 IP 데이터를 사용자 영역으로 비동기 전송하는 큐(Queue).
3.  **K8s Watcher (Go):** 쿠버네티스 API Server를 감시하여 `Service ClusterIP` ↔ `Namespace/ServiceName` 매핑 테이블을 실시간 동기화한다.
4.  **Metric Exporter (Go):** RingBuffer에서 넘어온 IP가 매핑 테이블에 존재할 경우, 해당 서비스의 호출 카운트를 증가시키고 `/metrics` 엔드포인트를 통해 노출한다.

-----

## 3\. 기능 요구사항 (Functional Requirements)

### FR-1. 커널 레벨 트래픽 감지 (eBPF)

  * **Hook Point:** `kprobe/tcp_v4_connect` (IPv4 TCP 연결 시도 함수)
  * **Data Extraction:** 소켓 구조체(`struct sock`) 또는 인자(`struct sockaddr_in`)에서 \*\*목적지 IPv4 주소(daddr)\*\*를 추출해야 한다.
  * **Performance:** 연결 시도(SYN) 시에만 동작해야 하며, 데이터 전송 패킷에는 관여하지 않아야 한다.
  * **Filter:** 커널단에서는 필터링을 최소화하고 모든 연결 시도를 User Space로 넘긴다 (복잡도 감소 및 안전성 확보).

### FR-2. 쿠버네티스 서비스 매핑 (K8s Watcher)

  * **API Watch:** `CoreV1` 그룹의 `Service` 리소스를 감시(Watch)해야 한다.
  * **Mapping Table:** 메모리 내에 `Map[string]ServiceMetadata` 구조를 유지해야 한다.
      * Key: `ClusterIP` (예: "10.96.0.50")
      * Value: `{Namespace: "default", Name: "my-backend"}`
  * **Concurrency:** eBPF 이벤트 처리 루틴(Read)과 K8s 감시 루틴(Write)이 동시에 맵에 접근하므로 **Thread-safe (RWMutex)** 해야 한다.

### FR-3. 메트릭 노출 (Prometheus Exporter)

  * **Endpoint:** `0.0.0.0:8080/metrics`
  * **Metric Name:** `zero_watchdog_internal_tcp_attempts_total` (Counter)
  * **Labels:**
      * `destination_service`: 타겟 서비스 이름
      * `destination_namespace`: 타겟 네임스페이스
  * **Logic:** RingBuffer에서 꺼낸 IP가 Mapping Table에 **있을 때만** 카운트를 증가시킨다. (외부 IP나 파드 간 통신은 무시)

-----

## 4\. 데이터 명세 (Data Specification)

### 4.1 eBPF Event Struct (C -\> Go)

커널에서 Go로 넘기는 데이터 구조체입니다.

```c
struct event {
    u32 daddr; // Destination IPv4 Address (Big Endian)
};
```

### 4.2 Prometheus Metric Example

KEDA가 실제로 쿼리하게 될 메트릭의 형태입니다.

```text
# HELP zero_watchdog_internal_tcp_attempts_total Total count of internal TCP connection attempts targeted at Services
# TYPE zero_watchdog_internal_tcp_attempts_total counter
zero_watchdog_internal_tcp_attempts_total{destination_namespace="default", destination_service="backend-api"} 5
zero_watchdog_internal_tcp_attempts_total{destination_namespace="payment", destination_service="payment-db"} 12
```

-----

## 5\. 기술 스택 및 개발 환경 (Tech Stack)

| 구분 | 기술 / 도구 | 버전 / 비고 |
| :--- | :--- | :--- |
| **Language (User)** | Go (Golang) | 1.20 이상 |
| **Language (Kernel)** | C (Restricted C) | eBPF용 |
| **Compiler** | Clang / LLVM | 12 이상 (BPF 타겟 컴파일용) |
| **Library** | `cilium/ebpf` | Go 기반 eBPF 로더 및 유틸리티 |
| **Library** | `client-go` | Kubernetes API 연동 |
| **Library** | `prometheus/client_golang` | 메트릭 서버 구현 |
| **Build Tool** | `bpf2go` | C 코드를 Go 파일로 변환 및 임베딩 |
| **Runtime** | Docker / Kubernetes | Privileged 모드 필수 |

-----

## 6\. 구현 시나리오 (Implementation Plan)

### Phase 1: eBPF 프로토타입 (C + Go Skeleton)

1.  `tcp_v4_connect`에 Kprobe를 거는 C 코드 작성.
2.  `bpf2go`를 사용하여 Go 바인딩 생성.
3.  Go에서 RingBuffer를 통해 IP 주소를 수신하고 로그로 찍어보는 기능 구현.
4.  **검증:** `curl <임의의 IP>` 실행 시 Go 로그에 해당 IP가 찍히는지 확인.

### Phase 2: K8s Watcher 구현

1.  `client-go`를 사용하여 In-Cluster Config 연동.
2.  Service 리소스 Informer 구현 (Add/Update/Delete 이벤트 핸들링).
3.  ClusterIP와 서비스명을 매핑하는 Thread-safe Map 구현.

### Phase 3: 메트릭 통합 및 배포

1.  Prometheus Registry 등록 및 HTTP 서버 구현.
2.  수신된 IP를 Map과 대조하여 메트릭 카운팅 로직 연결.
3.  `Dockerfile` 작성 (Multi-stage build).
4.  Kubernetes `DaemonSet` 매니페스트 작성.

-----

## 7\. 제약 사항 및 리스크 (Constraints & Risks)

1.  **권한 (Privilege):** eBPF 프로그램을 로드하려면 컨테이너에 `privileged: true` 또는 `CAP_BPF`, `CAP_SYS_ADMIN` 등의 권한이 필요하다. (보안 정책 확인 필요)
2.  **커널 호환성:** `CO-RE (Compile Once - Run Everywhere)`를 지향하지만, 너무 오래된 커널(v4.18 미만)에서는 동작하지 않을 수 있다. (Target Cluster의 커널 버전 확인 필요)
3.  **ClusterIP만 감지:** Headless Service(ClusterIP: None)의 경우 IP가 파드 IP로 바로 변환되므로 감지가 어려울 수 있다. (이번 PRD 범위에서는 **Standard ClusterIP Service**만 대상으로 한다.)